{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session #1: Sampling from a Gaussian Process\n",
    "\n",
    "#### Welcome to the first coding session of the workshop! \n",
    "\n",
    "In this session we will get a first intuition for for the definition of a Gaussian process. \n",
    "\n",
    "A Gaussian process can be seen as an infinite dimensional Gaussian distribiution - a distribution over functions. Like any other Gaussian distribution it is fully defined by a mean $m(x)$ and a covariance function $k(x, x')$: \n",
    "\\begin{align}\n",
    "f(x) \\sim \\mathcal{GP}( m(x), k(x, x'))\n",
    "\\end{align}\n",
    "\n",
    "The mean $m(x)$ just gives the mean of the random variable at $x$ for any $x \\in \\mathbb{R}$. The covariance function $k(x, x')$ gives the covariance of any two random variables $x$ and $x'$. It determines the autocorrelation length or the 'smoothness' of the functions defined by the Gaussian process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task: \n",
    "\n",
    "Your task is to \n",
    "\n",
    "a) define a Gaussian process, to \n",
    "\n",
    "b) draw function samples from it and to \n",
    "\n",
    "c) look at the distribution one or several random variables over samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Defining a Gaussian Process: \n",
    "\n",
    "To keep things simple, let us define the mean function to be just zero: $m(x) = 0$. Then, the only thing we still need is a covariance function. For a start we take the squared exponential covariance function: \n",
    "\n",
    "\\begin{align}\n",
    "k(x, x') = \\exp(- 0.5 |x - x'|^2)\n",
    "\\end{align}\n",
    "\n",
    "I will provide a function that takes any two arrays $x$ and $x'$ and returns the corresponding covariance matrix $K$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_covariance_matrix(x_p, x_q): \n",
    "\n",
    "    # for a convenient computation, we need a two dimensional array\n",
    "    if x_p.ndim < 2: \n",
    "        x_p = x_p.reshape(-1 ,1)\n",
    "    if x_q.ndim < 2: \n",
    "        x_q = x_q.reshape(-1, 1)\n",
    "    \n",
    "    # calculate the squared distance: x^2 - 2xy + y^2\n",
    "    square_dist = np.sum(x_p ** 2, 1).reshape(-1, 1) + np.sum(x_q ** 2, 1) - 2 * np.dot(x_p, x_q.T)\n",
    "    \n",
    "    # return the exponential of the squared distance\n",
    "    return np.exp(-0.5 * square_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some values x over a range. Make sure to samples densely for better visualization as a 'function'. \n",
    "\n",
    "\n",
    "# calculate the corresponding covariance matrix k(x, x')\n",
    "\n",
    "\n",
    "# test: plot the matrix using plt.imshow. it should be bright on the first diagonal and exponentially decaying. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sampling from a multivariat Gaussian distribution\n",
    "\n",
    "Samples from the Gaussian process are infinite objects in theory, but we will of course sample finite objects, i.e., we sample a long vector and treat it as a 'function'. So effectively, we want to sample from a multivariat Gaussian distribution. How do we do that? \n",
    "\n",
    "There is no simple thing like np.random.normal for a d-dimensional case. But I will provide a function that gives a $n$ samples from a $d$-dimensional Gaussian distribution with arbitrary mean and covariance functions. For details, see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_multivariat_gaussian(m, k, n_dimensions, n_samples): \n",
    "    \"\"\"\n",
    "    Draw samples from an arbitrary Gaussian distribution with mean m and covariance k. \n",
    "    \n",
    "    The dimensions of m and k have to correspond, e.g., \n",
    "    \"\"\"\n",
    "    # calculate cholesky decomposition. add a bit of noise for numerical stability\n",
    "    L = np.linalg.cholesky(k + 1e-6 * np.random.rand() * np.eye(n_dimensions))\n",
    "\n",
    "    # draw samples from a 1-D Gaussian distribution \n",
    "    u = np.random.normal(size=(n_dimensions, n_samples))\n",
    "\n",
    "    # use the cholesky decomposition to provide these samples with \n",
    "    # the covariance structure of the multidimensional Gaussian\n",
    "    return m + L.dot(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw some samples, e.g., n_samples=1000. \n",
    "# Use m=0 and the covariance matrix you defined above. What is the number of dimensions then? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is returned is a (n_dimensions, n_samples) shaped matrix. It contains your sampled functions! In which dimension of the matrix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot some of the sampled 'functions' \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice! You have just sampled functions from an infinite dimensional Gaussian distribution! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution \n",
    "\n",
    "Please double check! Plot the distrubtion over samples at a single point $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to see the effect make sure that you esed a lot of samples in the GP above, e.g, at least n_samples=1000. \n",
    "# plot a histogram of the samples at a single point x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the join distribution of two points $x_1$ and $x_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can use np.histrogram2d to get a matrix histogram from two vectors\n",
    "\n",
    "\n",
    "# and then you could use plt.imshow to plot this histogram matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well done! \n",
    "\n",
    "#### We have defined a Gaussian process as a collection of random variables and sampled from this collection. \n",
    "\n",
    "#### We demonstrated that any subset of this collection is a Gaussian distribution itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling using the Cholesky decomposition\n",
    "\n",
    "To generate samples $x \\sim \\mathcal{N}(m, K)$ with arbitrary mean **m** and covariance K using a Gaussian scalar generator (e.g., np.random.normal) we proceed as follows: \n",
    "\n",
    "We compute the Cholesky decomposition $L$ of the positive definite covariance matrix $K$, $K=LL^T$, where $L$ is a lower triangular matrix. Then generate $u \\sim \\mathcal{N}(0, I)$ Gaussian scalar samples. Compute $x = m + Lu$, which has the desired distribution with mean $m$ and covariance $L \\mathcal{E}[uu^T]L^T = LL^T = K$. \n",
    "\n",
    "In practice it may be necessary to add a small multiple of the identity matrix $\\epsilon I$ to the covariance matrix for numerical reasons. This is because the eigenvalues of the matrix $K$ can decay very rapidly (see section 4.3.1 for \n",
    "a closely related analytical result) and without this stabilization the Cholesky decomposition fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bccn_programming]",
   "language": "python",
   "name": "conda-env-bccn_programming-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
