{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session #2: Gaussian process regression\n",
    "\n",
    "#### Let us now actucally use a Gaussian process and do more than just sampling from it! \n",
    "\n",
    "We will define a regression model with a Gaussian process. \n",
    "\n",
    "The standard linear regression model uses training data $(X, y)$ to predict the value of the target variable from unseen data $x_*$ by learning the underlying function f(x): \n",
    "\n",
    "$$\n",
    "f(x) = x^T w\n",
    "$$\n",
    "\n",
    "We assume that the observations $y$ come with additive Gaussian noise on top, such that: \n",
    "\n",
    "$$\n",
    "y(x) = f(x) + \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)\n",
    "$$\n",
    "\n",
    "To find this underlying function we start off with a Gaussian process prior with zero mean and squared exponential covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task: \n",
    "\n",
    "I provide some training data $(X, y)$ over a given range. The $y-values$ are noisy observations of the underlying function $f$. \n",
    "Your task is to \n",
    "\n",
    "a) define the GP prior using the training and the testing $x-values$ \n",
    "\n",
    "b) condition the GP prior on the training data to get the predictive distribution\n",
    "\n",
    "c) play with the length scale of the covariance to get a good fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_covariance_matrix(x_p, x_q, l=.01): \n",
    "    # for a convenient computation, we need a two dimensional array\n",
    "    if x_p.ndim < 2: \n",
    "        x_p = x_p.reshape(-1 ,1)\n",
    "    if x_q.ndim < 2: \n",
    "        x_q = x_q.reshape(-1, 1)\n",
    "    # calculate the squared distance: x^2 - 2xy + y^2\n",
    "    square_dist = np.sum(x_p ** 2, 1).reshape(-1, 1) + np.sum(x_q ** 2, 1) - 2 * np.dot(x_p, x_q.T)\n",
    "    \n",
    "    # return the exponential of the squared distance\n",
    "    return np.exp((-0.5 / l) * square_dist)\n",
    "\n",
    "def secret_function(x, omega=2.): \n",
    "    return np.sin(omega * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here comes the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training data \n",
    "n_train = 20\n",
    "sigma_noise = .2\n",
    "xtrain = np.hstack((np.linspace(-5 , 2, n_train / 2), np.linspace(4 , 9, n_train / 2)))\n",
    "ytrain = secret_function(xtrain) + np.random.standard_normal(xtrain.size) * sigma_noise\n",
    "\n",
    "# we take many testing point for better visualization\n",
    "n_test = 1000\n",
    "xtest = np.linspace(-10, 10, n_test)\n",
    "ytest = secret_function(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(xtest, ytest, label='underlying function f(x)')\n",
    "plt.plot(xtrain, ytrain, 'o', label='training data')\n",
    "plt.title('Training data and underlying function')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Define the GP prior\n",
    "Because m(x) = 0 for the prior all you need to define the prior is the covariance matrix of the training data.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f} \\sim \\mathcal{GP}(\\mathbf{0}, k(\\mathbf{x, x'})) = \\mathcal{N}(0, K(X, X) + \\sigma_n^2 \\mathbf{I})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the training data to define the covariance matrix  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What now? This covariance matrix contains only the training data points $X$. To make predictions we need to incorporate the test data $X_*$ as well. So actually we need the joint covariance matrix of the training and the test data: \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\t\\mathbf{y} \\\\ \\mathbf{f_*}\n",
    "\\end{bmatrix}\n",
    " = \\mathcal{N}\\left(0, \n",
    "\t\\begin{bmatrix} \n",
    "\t\tK(X, X) + \\sigma_n^2 \\mathbf{I} & K(X,X_*) \\\\\n",
    "\t\tK(X_*,X) & K(X_*, X_*)\n",
    "\t\\end{bmatrix} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the joint prior \n",
    "# by combining training and test data covariance matrices into a single large matrix like in the equation above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now sample from this GP like before, but we don't. \n",
    "\n",
    "Rather, we will calculate the predictive distribution $f_*$ to make predictions for new values $x_*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Calculate the predictive distribution\n",
    "\n",
    "We get the predictive distribution by conditioning the joint prior on the training data $(X, y)$. The predictive distribution is again, guess what, a Gaussian: \n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{f}_*| X_*, X, \\mathbf{y} ) &\\sim \\mathcal{N} (m(\\mathbf{x}) , \\Sigma) \\\\\n",
    "\\end{align\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "m(\\mathbf{x}) &= K(X_*, X) [K(X, X) + \\sigma_n^2 \\mathbf{I}]^{-1}\\mathbf{y} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma &= K(X_*, X_*) - K(X_*, X) [K(X, X) + \\sigma_n^2 \\mathbf{I}]^{-1}K(X,X_*)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the formulas above to define the mean function and the covariance matrix of the predictive distribution\n",
    "# the mean function\n",
    "\n",
    "\n",
    "# the covariance matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the predictive distribution is Gaussian the mean and the covariance completely define our estimation of the underlying function $f$. The mean is our prediction and the variance at every sample is our certainty of the prediction. \n",
    "\n",
    "Plot the prediction for $f(x\\_test)$ and the corresponding variance or standard deviation at every position, e.g., as shaded aread around the prediction (check out plt.fill_between). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the standard deviation of each individual x_test from the covariance matrix: \n",
    "\n",
    "\n",
    "# plot the prediction m(x) \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# and the covariance for every x: plt.fill_between\n",
    "\n",
    "# plot the training data \n",
    "\n",
    "# plot the underlying function \n",
    "plt.title('Mean and variance of the predictive distr. with training data points');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Play with the length scale parameter of the covariance function ot the get a better fit. \n",
    "\n",
    "You can use the plotting function below if you want. It just takes the data and the mean and variance of the predictive distribution and plots the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the predicitve distribution with a better length scale parameter\n",
    "\n",
    "\n",
    "# plot the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are stuck check out the two functions below. They give a solution for a) and b) and let you solve c) by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gp_regression(xtrain, ytrain, xtest, sigma_noise=.1, l=.1): \n",
    "\n",
    "    # calculate the covariance matrix \n",
    "    k11 = calculate_covariance_matrix(xtrain, xtrain, l=l) + sigma_noise ** 2 * np.eye(xtrain.shape[0])\n",
    "    k12 = calculate_covariance_matrix(xtrain, xtest, l=l)\n",
    "    k22 = calculate_covariance_matrix(xtest, xtest, l=l)\n",
    "    k21 = calculate_covariance_matrix(xtest, xtrain, l=l)\n",
    "    \n",
    "    # Use the formulas above to define the mean function and the covariance matrix of the predictive distribution\n",
    "    # the mean function\n",
    "    invers_training_K = np.linalg.inv(k11)\n",
    "    m = k21.dot(invers_training_K).dot(ytrain)\n",
    "    # the covariance matrix. \n",
    "    sigma = k22 - k21.dot(invers_training_K).dot(k12)\n",
    "    \n",
    "    return m.squeeze(), sigma.squeeze()\n",
    "\n",
    "def plot_gp_regression_results(m, sigma, xtrain, ytrain, xtest, ytest): \n",
    "\n",
    "    std = np.sqrt(np.diag(sigma))\n",
    "    \n",
    "    upper_std = np.squeeze(m) + std\n",
    "    lower_std = np.squeeze(m) - std\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.fill_between(xtest, upper_std, lower_std, alpha=0.4)\n",
    "    plt.plot(xtest, m, 'r', label='Prediction mean')\n",
    "    plt.plot(xtrain, ytrain, 'go', label='data')\n",
    "    plt.plot(xtest, ytest)\n",
    "    plt.title('Mean and variance of the predictive distr. with training data points')\n",
    "    plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# calculate the mean and variance of the predictive distribution: \n",
    "cov_length_scale = .5\n",
    "mean, variance = gp_regression(xtrain, ytrain, xtest, sigma_noise=sigma_noise, l=cov_length_scale)\n",
    "\n",
    "# plot the results \n",
    "plot_gp_regression_results(mean, variance, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:bccn_programming]",
   "language": "python",
   "name": "conda-env-bccn_programming-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
